\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\begin{document}

\section{Dynamic State Estimation with State-Space Models}

\subsection{The State-Space Representation}
A linear discrete-time dynamic system is defined by two equations:
\begin{itemize}
    \item \textbf{Transition (State) Equation:} Describes the evolution of the unobserved state vector $\bm{x}_k \in \mathbb{R}^n$.
    \begin{equation}
        \bm{x}_{k+1} = \bm{\Phi} \bm{x}_k + \bm{w}_k, \quad \bm{w}_k \sim \mathcal{N}(\bm{0}, \bm{Q})
    \end{equation}
    where $\bm{\Phi}$ is the state transition matrix, and $\bm{w}_k$ is the process noise, assumed to be a zero-mean Gaussian white noise process with covariance matrix $\bm{Q}$.

    \item \textbf{Observation (Measurement) Equation:} Links the state vector to the observed measurements $\bm{z}_k \in \mathbb{R}^m$.
    \begin{equation}
        \bm{z}_k = \bm{H} \bm{x}_k + \bm{v}_k, \quad \bm{v}_k \sim \mathcal{N}(\bm{0}, \bm{R})
    \end{equation}
    where $\bm{H}$ is the observation matrix, and $\bm{v}_k$ is the measurement noise, a zero-mean Gaussian white noise process with covariance matrix $\bm{R}$. The noise processes $\bm{w}_k$ and $\bm{v}_k$ are assumed to be uncorrelated.
\end{itemize}

\subsection{The Kalman Filter: Optimal Recursive Estimation}
The Kalman filter provides the optimal linear estimate of the state $\bm{x}_k$ by minimizing the mean squared error (MSE). It operates in a two-step recursive cycle. Let $\hat{\bm{x}}_{k|j}$ be the estimate of $\bm{x}_k$ given observations up to time $j$, and let $\bm{P}_{k|j}$ be the corresponding error covariance matrix: $\bm{P}_{k|j} = E[(\bm{x}_k - \hat{\bm{x}}_{k|j})(\bm{x}_k - \hat{\bm{x}}_{k|j})^T]$.

\begin{itemize}
    \item \textbf{Time Update (Predict):} Projects the state and covariance estimates from time $k-1$ to $k$.
    \begin{align}
        \hat{\bm{x}}_{k|k-1} &= \bm{\Phi} \hat{\bm{x}}_{k-1|k-1} \label{eq:state_predict} \\
        \bm{P}_{k|k-1} &= \bm{\Phi} \bm{P}_{k-1|k-1} \bm{\Phi}^T + \bm{Q} \label{eq:cov_predict}
    \end{align}

    \item \textbf{Measurement Update (Correct):} Incorporates the new measurement $\bm{z}_k$ to refine the predicted estimates.
    \begin{align}
        \tilde{\bm{y}}_k &= \bm{z}_k - \bm{H} \hat{\bm{x}}_{k|k-1} & \text{(Innovation)} \\
        \bm{S}_k &= \bm{H} \bm{P}_{k|k-1} \bm{H}^T + \bm{R} & \text{(Innovation Covariance)} \\
        \bm{K}_k &= \bm{P}_{k|k-1} \bm{H}^T \bm{S}_k^{-1} & \text{(Optimal Kalman Gain)} \label{eq:kalman_gain} \\
        \hat{\bm{x}}_{k|k} &= \hat{\bm{x}}_{k|k-1} + \bm{K}_k \tilde{\bm{y}}_k & \text{(Updated State Estimate)} \label{eq:state_update} \\
        \bm{P}_{k|k} &= (\bm{I} - \bm{K}_k \bm{H}) \bm{P}_{k|k-1} & \text{(Updated Error Covariance)} \label{eq:cov_update}
    \end{align}
\end{itemize}

\subsubsection{Derivation of the Kalman Gain via MSE Minimization}
The updated state estimate $\hat{\bm{x}}_{k|k}$ is a linear combination of the prior estimate and the measurement: $\hat{\bm{x}}_{k|k} = \hat{\bm{x}}_{k|k-1} + \bm{K}_k (\bm{z}_k - \bm{H}\hat{\bm{x}}_{k|k-1})$. The error is $\bm{e}_{k|k} = \bm{x}_k - \hat{\bm{x}}_{k|k}$. Substituting the expressions for $\bm{z}_k$ and $\hat{\bm{x}}_{k|k}$ yields:
\begin{equation*}
    \bm{e}_{k|k} = (\bm{I} - \bm{K}_k \bm{H})(\bm{x}_k - \hat{\bm{x}}_{k|k-1}) - \bm{K}_k \bm{v}_k
\end{equation*}
The error covariance $\bm{P}_{k|k} = E[\bm{e}_{k|k}\bm{e}_{k|k}^T]$ is then:
\begin{equation}
    \bm{P}_{k|k} = (\bm{I} - \bm{K}_k \bm{H}) \bm{P}_{k|k-1} (\bm{I} - \bm{K}_k \bm{H})^T + \bm{K}_k \bm{R} \bm{K}_k^T
\end{equation}
To minimize the MSE, we minimize the trace of $\bm{P}_{k|k}$. We expand the expression for $\bm{P}_{k|k}$ and take the derivative of its trace with respect to $\bm{K}_k$:
\begin{equation*}
    \frac{d(\text{tr}(\bm{P}_{k|k}))}{d\bm{K}_k} = -2(\bm{H} \bm{P}_{k|k-1})^T + 2\bm{K}_k(\bm{H} \bm{P}_{k|k-1} \bm{H}^T + \bm{R})
\end{equation*}
Setting this to zero and solving for $\bm{K}_k$ yields the optimal Kalman gain in Equation \ref{eq:kalman_gain}. Substituting this optimal gain back into the equation for $\bm{P}_{k|k}$ simplifies to Equation \ref{eq:cov_update}.

\subsection{Steady-State Kalman Filter and the Algebraic Riccati Equation}
If the system matrices ($\bm{\Phi}, \bm{H}, \bm{Q}, \bm{R}$) are time-invariant and the system is detectable and stabilizable, the error covariance matrix $\bm{P}_{k|k-1}$ converges to a steady-state solution $\bm{P}$ as $k \to \infty$.
\begin{equation}
    \bm{P} = \lim_{k \to \infty} \bm{P}_{k|k-1}
\end{equation}
Substituting $\bm{P}$ into the recursive covariance equations (\ref{eq:cov_predict}) and (\ref{eq:cov_update}) yields the \textbf{Discrete Algebraic Riccati Equation (DARE)}:
\begin{equation}
    \bm{P} = \bm{\Phi} \left( \bm{P} - \bm{P}\bm{H}^T(\bm{H}\bm{P}\bm{H}^T + \bm{R})^{-1}\bm{H}\bm{P} \right) \bm{\Phi}^T + \bm{Q}
\end{equation}
Solving this equation allows for the use of a constant, pre-computed steady-state Kalman gain $\bm{K} = \bm{P}\bm{H}^T(\bm{H}\bm{P}\bm{H}^T + \bm{R})^{-1}$.

\section{Cointegration and Error-Correction Models}

\subsection{Cointegration and the Error-Correction Model (ECM)}
Two or more time series, $p_{1,t}, \dots, p_{n,t}$, each integrated of order one (I(1)), are cointegrated if there exists a vector $\bm{\beta} = (1, -\beta_2, \dots, -\beta_n)$ such that the linear combination $s_t = \bm{\beta}' \bm{p}_t$ is stationary (I(0)). The vector $\bm{\beta}$ is the cointegrating vector.
The Granger Representation Theorem implies that a cointegrated system can be represented by an \textbf{Error-Correction Model (ECM)}. For a bivariate system $(p_{1,t}, p_{2,t})$:
\begin{equation}
    \Delta p_{1,t} = \alpha_1 (p_{1,t-1} - \beta_0 - \beta_1 p_{2,t-1}) + \sum_{j=1}^{p} \gamma_{11,j} \Delta p_{1,t-j} + \sum_{j=1}^{p} \gamma_{12,j} \Delta p_{2,t-j} + \epsilon_{1,t}
\end{equation}
Here, $(p_{1,t-1} - \beta_0 - \beta_1 p_{2,t-1})$ is the error-correction term, representing the deviation from the long-run equilibrium in the previous period. The coefficient $\alpha_1$ is the speed of adjustment.

\subsection{Statistical Tests for Cointegration}
\subsubsection{Augmented Dickey-Fuller (ADF) Test}
Used to test for a unit root in a time series. The regression model is:
\begin{equation}
    \Delta y_t = \mu + \gamma t + \delta y_{t-1} + \sum_{i=1}^{p} \phi_i \Delta y_{t-i} + \epsilon_t
\end{equation}
The null hypothesis of a unit root is $H_0: \delta = 0$ against the alternative of stationarity $H_A: \delta < 0$. The test statistic is the t-statistic for $\hat{\delta}$, which follows a non-standard distribution.

\subsubsection{Johansen Test}
This test is conducted within a Vector Error Correction Model (VECM) framework:
\begin{equation}
    \Delta \bm{y}_t = \bm{\Pi} \bm{y}_{t-1} + \sum_{i=1}^{p-1} \bm{\Gamma}_i \Delta \bm{y}_{t-i} + \bm{\epsilon}_t
\end{equation}
where $\bm{\Pi} = \bm{\alpha}\bm{\beta}'$. The rank of $\bm{\Pi}$, denoted $r$, is the number of cointegrating relationships.
\begin{itemize}
    \item \textbf{Trace Test:} Tests $H_0: \text{rank}(\bm{\Pi}) \le r$ vs $H_A: \text{rank}(\bm{\Pi}) > r$. The statistic is:
    \begin{equation}
        \lambda_{\text{trace}}(r) = -T \sum_{i=r+1}^{n} \ln(1 - \hat{\lambda}_i)
    \end{equation}
    \item \textbf{Maximum Eigenvalue Test:} Tests $H_0: \text{rank}(\bm{\Pi}) = r$ vs $H_A: \text{rank}(\bm{\Pi}) = r+1$. The statistic is:
    \begin{equation}
        \lambda_{\text{max}}(r, r+1) = -T \ln(1 - \hat{\lambda}_{r+1})
    \end{equation}
\end{itemize}
where $\hat{\lambda}_i$ are the ordered eigenvalues from the canonical correlation analysis. Critical values are non-standard and depend on the deterministic terms included.

\subsubsection{Finite-Sample Corrections}
Asymptotic critical values can be poor approximations. Corrections exist:
\begin{itemize}
    \item \textbf{ADF Test:} Response surface regressions provide more accurate critical values that depend on sample size $T$ and lag order $k$.
    \item \textbf{Johansen Test:} A Bartlett correction factor can be applied to the test statistics to better align their finite-sample moments with their asymptotic counterparts.
\end{itemize}

\section{Change-Point Detection and Model Adaptation}

\subsection{CUSUM Algorithm}
The Cumulative Sum (CUSUM) algorithm detects shifts in the mean of a process. For detecting an upward shift from a target mean $\mu_0$:
\begin{equation}
    S_t = \max(0, S_{t-1} + (y_t - \mu_0 - k))
\end{equation}
where $k$ is a slack parameter (often half the expected shift size) and $S_0 = 0$. A change is signaled if $S_t > h$ for a predefined threshold $h$.

\subsection{Bayesian Online Change-Point Detection (BOCPD)}
BOCPD computes the posterior probability of the run length $r_t$ (time since the last change point) at each step. The update rule is:
\begin{equation}
    P(r_t | \bm{x}_{1:t}) \propto \sum_{r_{t-1}} P(\bm{x}_t | r_t, \bm{x}_{t-1}^{(r_{t-1})}) P(r_t | r_{t-1}) P(r_{t-1} | \bm{x}_{1:t-1})
\end{equation}
The transition prior $P(r_t | r_{t-1})$ is composed of a changepoint probability $H(r_{t-1}+1)$ (where $r_t=0$) and a growth probability $1-H(r_{t-1}+1)$ (where $r_t=r_{t-1}+1$).

\subsection{Adapting Kalman Filter State Noise}
Upon detecting a regime shift, the process noise covariance $\bm{Q}$ can be adapted. One method is innovation-based adaptive estimation. The theoretical covariance of the innovation is $\bm{S}_k = \bm{H} \bm{P}_{k|k-1} \bm{H}^T + \bm{R}$. The empirical covariance over a recent window of size $M$ is $\hat{\bm{C}}_{\tilde{y}, k} = \frac{1}{M} \sum_{j=k-M+1}^{k} \tilde{\bm{y}}_j \tilde{\bm{y}}_j^T$. A mismatch suggests $\bm{Q}$ is incorrect. An adaptive update for $\bm{Q}$ is:
\begin{equation}
    \hat{\bm{Q}}_k = \bm{K}_k (\hat{\bm{C}}_{\tilde{y}, k} - \bm{H}\bm{P}_{k|k-1}^-\bm{H}^T - \bm{R}) \bm{K}_k^T
\end{equation}
where $\bm{P}_{k|k-1}^-$ is the prior covariance calculated with the old $\bm{Q}$. More simply, a heuristic is to scale $\bm{Q}$ by a factor $\lambda > 1$ to increase filter responsiveness.  Decision thresholds for trading strategies can also be adjusted based on regime. For example, wider spreads might be required for entry/exit during high volatility regimes.

\section{Overfitting Controls}

\subsection{Multiple Testing Correction: Bonferroni-Holm Method}
To control the Family-Wise Error Rate (FWER) at level $\alpha$ when conducting $m$ hypothesis tests, sort the p-values $p_{(1)} \le p_{(2)} \le \dots \le p_{(m)}$. For $i = 1, \dots, m$, reject the null hypothesis $H_{(i)}$ if:
\begin{equation}
    p_{(i)} \le \frac{\alpha}{m - i + 1}
\end{equation}
Stop and fail to reject all subsequent hypotheses at the first instance this condition is not met.

\subsection{Purged K-Fold Cross-Validation and Walk-Forward Analysis}
Standard k-fold cross-validation is unsuitable for time series data due to temporal dependencies. Purged k-fold CV addresses this by removing data points from the training set that are temporally close to the test set, preventing leakage. Walk-forward analysis simulates real-world trading by training on past data and testing on future data in a rolling window fashion.

\subsection{Latency-Aware Feature Engineering}
In high-frequency trading, feature computation latency is crucial. Features must be calculable quickly from readily available data. Examples include:
\begin{itemize}
    \item Order Book Imbalance: (Bid Volume - Ask Volume) / (Bid Volume + Ask Volume)
    \item Micro-Price: (Best Bid * Ask Size + Best Ask * Bid Size) / (Bid Size + Ask Size)
    \item Trade Flow Imbalance: Aggregated buy volume - aggregated sell volume over a short window
\end{itemize}


\end{document}